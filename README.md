<div align="right">
  <a href="README_EN.md">ğŸŒ English Version</a>
</div>

# ğŸš€FastGPT-vLLM-Qwen-TEIå…¨é“¾è·¯æœ¬åœ°åŒ–éƒ¨ç½²

> åŸºäºvLLM(Qwen2.5)çš„FastGPTé«˜æ€§èƒ½ç§æœ‰RAGçŸ¥è¯†åº“å’ŒFrp+Nginxå…¬ç½‘è®¿é—®çš„ä¸€ç«™å¼éƒ¨ç½²æ–¹æ¡ˆã€‚
> ä¸“ä¸ºå›½å†…å†…ç½‘ç¯å¢ƒä¼˜åŒ–ï¼Œè§£å†³HuggingFaceä¸‹è½½ã€Dockeré•œåƒåŠ é€ŸåŠæ˜¾å­˜OOMç­‰ç—›ç‚¹ã€‚

![FastGPT](https://img.shields.io/badge/FastGPT-v4.x-blue)
![vLLM](https://img.shields.io/badge/vLLM-0.6.x-green)
![Qwen](https://img.shields.io/badge/Model-Qwen2.5--7B-violet)
![TEI](https://img.shields.io/badge/Embedding-BGE--M3-orange)

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æä¾›äº†ä¸€å¥—å®Œæ•´çš„FastGPTæœ¬åœ°åŒ–éƒ¨ç½²å®æˆ˜æŒ‡å—ï¼ŒåŸºäºvLLMï¼ˆæ¨ç†åŠ é€Ÿï¼‰å’ŒTEIï¼ˆå‘é‡åŠ é€Ÿï¼‰æ„å»ºï¼Œå……åˆ†å‘æŒ¥å•å¡æ˜¾å­˜æ€§èƒ½ï¼Œå®ç°é«˜å¹¶å‘ã€ä½å»¶è¿Ÿçš„çŸ¥è¯†åº“é—®ç­”ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ğŸ‡¨ğŸ‡³ **å›½å†…ç½‘ç»œä¼˜åŒ–**ï¼šæä¾›HuggingFaceé•œåƒä¸‹è½½ã€Dockeré•œåƒä»£ç†é…ç½®å…¨æµç¨‹ã€‚
- âš¡ **é«˜æ€§èƒ½æ¨ç†**ï¼šä½¿ç”¨vLLMéƒ¨ç½²Qwen2.5-7Bï¼Œä½¿ç”¨TEIéƒ¨ç½²BGE-M3ã€‚
- ğŸ’¾ **æ˜¾å­˜ç²¾ç»†ç®¡ç†**ï¼šé€šè¿‡å‚æ•°è°ƒä¼˜ï¼Œå®ç°LLMä¸Embeddingæ¨¡å‹åœ¨å•å¡æ˜¾å­˜ä¸‹çš„ç¨³å®šå…±å­˜ã€‚
- ğŸŒ **å…¬ç½‘è®¿é—®**ï¼šé›†æˆFRP + Nginxæ–¹æ¡ˆï¼Œå®ç°å†…ç½‘æœåŠ¡çš„å…¬ç½‘å®‰å…¨è®¿é—®ã€‚

## ğŸ› ï¸ æŠ€æœ¯æ ˆ
- **åº”ç”¨å±‚**ï¼šFastGPT + MongoDB + PostgreSQL(pgvector)
- **æ¨ç†å±‚**ï¼švLLM(Qwen2.5-7B-Instruct)
- **ç´¢å¼•å±‚**ï¼šText Embeddings Inference(BAAI/bge-m3)
- **ç½‘ç»œå±‚**ï¼šFRP(å†…ç½‘ç©¿é€) + Nginx(åå‘ä»£ç†)
- **å®¹å™¨åŒ–**ï¼šDocker & Docker Compose

## âš¡ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè¦æ±‚
- Ubuntu 20.04 / 22.04
- CUDA 12.1+
- æ˜¾å­˜ â‰¥ 24GB (å®é™…å¯ä»¥ä¸ç”¨è¿™ä¹ˆå¤§ï¼Œæ ¹æ®è‡ªå·±æ˜¾å­˜è°ƒèŠ‚æ¨¡å‹å³å¯)

### 2. å›½å†…åŠ é€Ÿæ¨¡å‹ä¸‹è½½

```bash
pip install huggingface_hub
export HF_ENDPOINT=https://hf-mirror.com

# Qwen2.5, å°†'/your/path/'åˆ‡æ¢æˆä½ å®é™…ä¸‹è½½åˆ°æœ¬åœ°çš„è·¯å¾„
huggingface-cli download --resume-download Qwen/Qwen2.5-7B-Instruct \
  --local-dir /your/path/Qwen2.5 

# BGE-M3, å°†'/your/path/'åˆ‡æ¢æˆä½ å®é™…ä¸‹è½½åˆ°æœ¬åœ°çš„è·¯å¾„
huggingface-cli download --resume-download BAAI/bge-m3 \
  --local-dir /your/path/bge-m3 \
  --exclude "*.DS_Store"
```

### 3.å¯åŠ¨æœ¬åœ°æ¨¡å‹æœåŠ¡

### 3.1å¯åŠ¨vLLM(å¯¹è¯æ¨¡å‹æœåŠ¡)
- æ³¨æ„ï¼šæœ¬é¡¹ç›®ä¸ºäº†ä¿è¯å•å¡24Gæ˜¾å­˜èƒ½å¤ŸåŒæ—¶è¿è¡ŒEmbeddingæ¨¡å‹ï¼Œæˆ‘å°†vLLMçš„æ˜¾å­˜åˆ©ç”¨ç‡ä¸¥æ ¼é™åˆ¶åœ¨80%ï¼Œå®é™…å¯æ ¹æ®è‡ªå·±æ˜¾å¡çš„æ˜¾å­˜æ¥è°ƒæ•´è¿™ä¸ªæ¯”ä¾‹ã€‚
```bash
# å°†/your/path/Qwen2.5æ›¿æ¢ä¸ºæ‚¨å®é™…çš„æ¨¡å‹ä¸‹è½½è·¯å¾„
python -m vllm.entrypoints.openai.api_server \
    --model /your/path/Qwen2.5 \
    --served-model-name Qwen2.5 \
    --max-model-len 8192 \
    --gpu-memory-utilization 0.8 \
    --port 8000 \
    --host 0.0.0.0 \
    --api-key sk-123456
```
- --served-model-name Qwen2.5: å…³é”®å‚æ•°ï¼Œå¼ºåˆ¶æŒ‡å®šæ¨¡å‹åç§°ï¼Œé˜²æ­¢FastGPTæŠ¥é”™404ã€‚
- --gpu-memory-utilization 0.8: é¢„ç•™çº¦20%æ˜¾å­˜ç»™TEIæ¨¡å‹ä½¿ç”¨ã€‚

### 3.2 å¯åŠ¨TEI(å‘é‡ç´¢å¼•æœåŠ¡)
- æ³¨æ„ï¼šæœ¬é¡¹ç›®ä½¿ç”¨DockeræŒ‚è½½æœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œå½»åº•è§„é¿å®¹å™¨å†…ä¸‹è½½å¤±è´¥çš„é—®é¢˜ã€‚
```bash
# å°†/your/path/bge-m3æ›¿æ¢ä¸ºæ‚¨å®é™…çš„æ¨¡å‹ä¸‹è½½è·¯å¾„
# ä½¿ç”¨m.daocloud.ioä»£ç†ghcr.ioé•œåƒ
sudo docker run --gpus all -d --name tei-bge \
    -p 8008:80 \
    -v /your/path/bge-m3:/data \
    m.daocloud.io/ghcr.io/huggingface/text-embeddings-inference:latest \
    --model-id /data
```

